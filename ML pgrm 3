

import pandas as pd
df_tennis=pd.read_csv('lab3_2.csv')
attribute_names = list(df_tennis.columns)
attribute_names.remove('play')


def entropy_of_list(a_list):
  from collections import Counter
  cnt = Counter(x for x in a_list)
  num_instances = len(a_list)*1.0
  probs = [x/num_instances for x in cnt.values()]
  return entropy(probs)


def entropy(probs):
  import math
  return sum([-prob*math.log(prob,2) for prob in probs])


total_entropy = entropy_of_list(df_tennis['play'])


def information_gain(df,split_attribute_name,target_attribute_name,trace=0):
  df_split = df.groupby(split_attribute_name)
  nobs = len(df.index)*1.0
  df_agg_ent = df_split.agg({target_attribute_name:[entropy_of_list,lambda x:len(x)/nobs]})
  df_agg_ent.columns = ['Entropy','propobservations']
  new_entropy = sum(df_agg_ent['Entropy']*df_agg_ent['propobservations'])
  old_entropy = entropy_of_list(df[target_attribute_name])
  return old_entropy-new_entropy


def id3(df,target_attribute_name,attribute_names,default_class = None):
  from collections import Counter
  cut = Counter(x for x in df[target_attribute_name])
  if len(cut) == 1:
    return next(iter(cut))
  elif df.empty or (not attribute_names):
    return default_class
  else:
    default_class = max(cut.keys())
    gainz = [information_gain(df,attr,target_attribute_name) for attr in attribute_names]
    index_of_max = gainz.index(max(gainz))
    best_attr = attribute_names[index_of_max]
    tree = {best_attr:{}}
    remaining_attribute_names = [i for i in attribute_names if i!=best_attr]
    for attr_val,data_subset in df.groupby(best_attr):
      subtree = id3(data_subset,target_attribute_name,remaining_attribute_names,default_class)
      tree[best_attr][attr_val] = subtree
  return tree



from pprint import pprint
tree = id3(df_tennis,'play',attribute_names)
print("\n\n The Resultant Decission Tree is :\n")
pprint(tree)

